{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifiers\n",
    "\n",
    "In this example, we will construct a simple binary classifier. Let's first look at our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples = np.random.multivariate_normal([-1, 1], [[0.1, 0], [0, 0.1]], 100)\n",
    "b_samples = np.random.multivariate_normal([1, -1], [[0.1, 0], [0, 0.1]], 100)\n",
    "a_targets = np.zeros(100)  # Samples from class A are assigned a class value of 0.\n",
    "b_targets = np.ones(100)  # Samples from class B are assigned a class value of 1.\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we can image a line that separates these two sets of data cleanly. Samples appearing on one side of the line are assigned to one class, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100)\n",
    "y = x\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y, c='g')\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are looking for is a function $y = f(\\mathbf{x})$ that maps the features in $\\mathbf{x}$ to a classification (either 0 or 1). The data we generated above is two-dimensional, so our function should consider both features of each sample.\n",
    "\n",
    "The linear classifier we will use takes the form of $y = f(\\mathbf{x}; \\mathbf{w})$, where $\\mathbf{x} = (x_1, x_2)$ is the sample and its features and $\\mathbf{w} = (w_1, w_2)$ are the parameters of our classifier. Formally, a linear classifier computes a linear combination of the input and coefficients, $f(\\mathbf{x}; \\mathbf{w}) = \\mathbf{w} \\cdot \\mathbf{x} = \\sum_j w_j x_j.$\n",
    "\n",
    "Notice that we have two variables in the input as well as two corresponding parameters of our classifier. We can arrange this in the form a line $ax + by + c = 0$. \n",
    "\n",
    "For our samples $\\mathbf{x}$ and $\\mathbf{w}$, the equation is $w_1 x_1 + w_2 x_2 - b = 0$. The previous coefficient $c$ has been renamed $b$ and will serve as our bias. We will see why this is important in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_decision_boundary(weights, bias):\n",
    "    m = -weights[1] / weights[0]\n",
    "    b = -bias\n",
    "    return np.array([m, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Parameters\n",
    "weights = np.array([0, 0]) \n",
    "bias = 1\n",
    "\n",
    "# For visualizing the line\n",
    "m, b = calc_decision_boundary(weights, bias)\n",
    "\n",
    "# If the slope is undefined, it is vertical.\n",
    "if weights[1] != 0:\n",
    "    x = np.linspace(-1, 1, 100)\n",
    "    y = m * x + b\n",
    "else:\n",
    "    x = np.zeros(100) + b\n",
    "    y = np.linspace(-1, 1, 100)\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y, c='g')\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually we can see that our linear classifier is well suited for this dataset. **How do we show this quantitatively?**\n",
    "\n",
    "For a binary classifier, if $$\\mathbf{wx} > 0$$ then we assign the sample $x$ to class 1. Otherwise, we will assign it to class 0. Classifiers are typically measured by their error rate. This is calculated by comparing the predictions versus the ground truth targets. Error measures are typically called loss functions. For this example, we will use squared L2 loss: $L_2 = \\frac{1}{2} \\sum_{i} (\\hat{y}_i - y_i)^2$, where $\\hat{y}_i$ is the ground truth target associated with sample $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear combination of weights and input\n",
    "y_a = weights @ a_samples.T + bias\n",
    "y_b = weights @ b_samples.T + bias\n",
    "\n",
    "# Step-wise activation function\n",
    "pred_a = y_a\n",
    "pred_b = y_b\n",
    "pred_a[pred_a < 0] = 0\n",
    "pred_b[pred_b >= 0] = 1\n",
    "\n",
    "l2_a = 0.5 * ((a_targets - pred_a)**2)\n",
    "l2_b = 0.5 * ((b_targets - pred_b)**2)\n",
    "loss_a = l2_a.sum()\n",
    "loss_b = l2_b.sum()\n",
    "print(\"Loss A = {}\".format(loss_a))\n",
    "print(\"Loss B = {}\".format(loss_b))\n",
    "\n",
    "# Combine and normalize the error between 0 and 1.\n",
    "loss = np.concatenate((l2_a, l2_b)).mean()\n",
    "print(\"Normalized loss = {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "The linear classifier we constructed above is also called a Perceptron. The step-wise activation function we used is simple: prediction values less than 0 are assigned to class A, all other values are assigned to B.\n",
    "\n",
    "# Non-linear Functions\n",
    "\n",
    "Instead of the step-wise function, let's evaluate the output by using a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear combination of weights and input\n",
    "y_a = weights @ a_samples.T + bias\n",
    "y_b = weights @ b_samples.T + bias\n",
    "\n",
    "# Sigmoid function\n",
    "pred_a = sigmoid(y_a)\n",
    "pred_b = sigmoid(y_b)\n",
    "print(pred_a)\n",
    "\n",
    "l2_a = 0.5 * ((a_targets - pred_a)**2)\n",
    "l2_b = 0.5 * ((b_targets - pred_b)**2)\n",
    "loss_a = l2_a.sum()\n",
    "loss_b = l2_b.sum()\n",
    "print(\"Loss A = {}\".format(loss_a))\n",
    "print(\"Loss B = {}\".format(loss_b))\n",
    "\n",
    "# Combine and normalize the error between 0 and 1.\n",
    "loss = np.concatenate((l2_a, l2_b)).mean()\n",
    "print(\"Normalized loss = {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened to our loss? Our classifier that previously had 0 error is now higher. Recall that we must treat this as a probability. Our classifier now answers this question: **what is the probability that this sample belongs to class B (because B is associated with 1)?**\n",
    "\n",
    "Note that we could still apply a step-wise function on top of this. If the classifier outputs a value of 0.9 for a given sample, is that sufficient to classify it as class B? What about 0.8, 0.7, 0.6, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
