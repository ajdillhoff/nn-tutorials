{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifiers\n",
    "\n",
    "In this example, we will construct a simple binary classifier. Let's first look at our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples = np.random.multivariate_normal([-1, 1], [[0.1, 0], [0, 0.1]], 100)\n",
    "b_samples = np.random.multivariate_normal([1, -1], [[0.1, 0], [0, 0.1]], 100)\n",
    "a_targets = np.zeros(100)  # Samples from class A are assigned a class value of 0.\n",
    "b_targets = np.ones(100)  # Samples from class B are assigned a class value of 1.\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we can image a line that separates these two sets of data cleanly. Samples appearing on one side of the line are assigned to one class, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "y = x\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y, c='g')\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')\n",
    "ax.set_xlim([-2, 2])\n",
    "ax.set_ylim([-2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are looking for is a function $y = f(\\mathbf{x})$ that maps the features in $\\mathbf{x}$ to a classification (either 0 or 1). The data we generated above is two-dimensional, so our function should consider both features of each sample.\n",
    "\n",
    "# Weights and Features\n",
    "The linear classifier we will use takes the form of $y = f(\\mathbf{x}; \\mathbf{w})$, where $\\mathbf{x} = (x_1, x_2)$ is the sample and its features and $\\mathbf{w} = (w_1, w_2)$ are the parameters of our classifier. Formally, a linear classifier computes a linear combination of the input and coefficients, $f(\\mathbf{x}; \\mathbf{w}) = \\mathbf{w} \\cdot \\mathbf{x} = \\sum_i w_i x_i.$\n",
    "\n",
    "**Our perceptron has 3 parameters**\n",
    "\n",
    "## Initialization\n",
    "What values should the parameters of our model start with? Typically, weights are randomly initialized using a variety of different techniques. For the purposes of this example, we will sample from a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Initialization\n",
    "weights = np.random.uniform(-1, 1, size=(3,))\n",
    "print(\"Weights: {}\".format(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass\n",
    "Calculating the output of a neural network is what is known as a **forward pass**. For our single layer perceptron, this is simply $y = h(\\mathbf{w}\\cdot\\mathbf{x})$.\n",
    "\n",
    "Let's implement this in Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    out = x.copy()\n",
    "    out[x < 0] = 0\n",
    "    out[x >= 0] = 1\n",
    "    return out\n",
    "\n",
    "def dot(w, x):\n",
    "    x_bias = np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)\n",
    "    return w @ x_bias.T\n",
    "\n",
    "# Forward pass -- use input from the blue distribution centered at (-1, 1)\n",
    "x = np.array([[-1.0, 1.0]])\n",
    "y = dot(weights, x)\n",
    "print(\"Before step function: {}\".format(y[0]))\n",
    "\n",
    "# Step function\n",
    "out = step(y)\n",
    "print(\"Final prediction: {}\".format(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Boundary\n",
    "\n",
    "Written out fully, the linear combination of the perceptron is:\n",
    "$y = h(w_0 + w_1 x_1 + w_2 x_2)$\n",
    "\n",
    "Notice that we have two variables in the input as well as two corresponding parameters of our classifier. We can arrange this in the form a line $Ax + By = C$. \n",
    "\n",
    "For our samples $\\mathbf{x}$ and $\\mathbf{w}$, the equation is $w_1 x_1 + w_2 x_2 - w_0 = 0$. The previous coefficient $C$ has been renamed $w_0$ and will serve as our bias. We will see why this is important in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_decision_boundary(weights):\n",
    "    x = -weights[0] / weights[1]\n",
    "    y = -weights[0] / weights[2]\n",
    "    m = -y / x\n",
    "    return np.array([m, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Parameters\n",
    "# print(weights)\n",
    "weights = np.array([0.1, 0.91290713, -0.19996809]) \n",
    "\n",
    "# For visualizing the line\n",
    "m, b = calc_decision_boundary(weights)\n",
    "print(\"Slope: {}\\nY-Intercept: {}\".format(m, b))\n",
    "\n",
    "# If the slope is undefined, it is vertical.\n",
    "if weights[2] != 0:\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = m * x + b\n",
    "else:\n",
    "    x = np.zeros(100)\n",
    "    y = np.linspace(-3, 3, 100) + b\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y, c='g')\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')\n",
    "ax.set_xlim([-2, 2])\n",
    "ax.set_ylim([-2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually we can see that our linear classifier is well suited for this dataset. **How do we show this quantitatively?**\n",
    "\n",
    "For a binary classifier, if $$\\mathbf{w}\\cdot\\mathbf{x} \\geq 0$$ then we assign the sample $x$ to class 1. Otherwise, we will assign it to class 0. Classifiers are typically measured by their error rate. This is calculated by comparing the predictions versus the ground truth targets. Error measures are typically called loss functions. For this example, we will use L1 loss: $L_1 = \\sum_{i} |\\hat{y}_i - y_i|$, where $\\hat{y}_i$ is the ground truth target associated with sample $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(pred, target):\n",
    "    return np.abs(target - pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear combination of weights and input\n",
    "y_a = dot(weights, a_samples)\n",
    "y_b = dot(weights, b_samples)\n",
    "\n",
    "# Step-wise activation function\n",
    "a_pred = step(y_a)\n",
    "b_pred = step(y_b)\n",
    "\n",
    "l1_a = l1_loss(a_pred, a_targets)\n",
    "l1_b = l1_loss(b_pred, b_targets)\n",
    "loss_a = l1_a.sum()\n",
    "loss_b = l1_b.sum()\n",
    "print(\"Loss A = {}\".format(loss_a))\n",
    "print(\"Loss B = {}\".format(loss_b))\n",
    "\n",
    "# Combine and normalize the error between 0 and 1.\n",
    "loss = np.concatenate((l1_a, l1_b)).mean()\n",
    "print(\"Normalized loss = {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Functions\n",
    "\n",
    "Instead of the step-wise function, let's evaluate the output by using a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear combination of weights and input\n",
    "y_a = dot(weights, a_samples)\n",
    "y_b = dot(weights, b_samples)\n",
    "\n",
    "print(y_a)\n",
    "\n",
    "# Sigmoid function\n",
    "pred_a = sigmoid(y_a)\n",
    "pred_b = sigmoid(y_b)\n",
    "\n",
    "print(pred_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened to our loss? Our classifier that previously had 0 error is now higher. Recall that we must treat this as a probability. Our classifier now answers this question: **what is the probability that this sample belongs to class B (because B is associated with 1)?**\n",
    "\n",
    "Note that we could still apply a step-wise function on top of this. If the classifier outputs a value of 0.9 for a given sample, is that sufficient to classify it as class B? What about 0.8, 0.7, 0.6, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
